{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 简答题\n",
    "1. 如果训练集有100万个实例，训练决策树（无约束）大致的深度是多少？\n",
    "2. 通常来说，子节点的基尼杂质是高于还是低于其父节点？是通常更高/更低？还是永远更高/更低？\n",
    "3. 如果决策树过拟合训练集，减少max_depth是否为一个好主意？\n",
    "4. 如果决策树对训练集欠拟合，尝试缩放输入特征是否为一个好主意？\n",
    "5. 如果在给定的训练集上训练决策树需要一个小时，那么如果将特征数量变为两倍，训练大约需要多少时间？\n",
    "6. 如果在包含100万个实例的训练集上训练决策树需要一个小时，那么在包含1000万个实例的训练集上训练决策树，大概需要多长时间？提示：考虑CART算法的计算复杂度。"
   ],
   "id": "c8b56739308b5b88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1：无约束的决策树会持续分裂，直到所有叶节点的实例 “纯”（即每个叶节点仅含一个实例，或所有实例属于同一类）\n",
    "# 2：通常更低，且永远更低\n",
    "# 3：过拟合的决策树因过度复杂（如深度过大、叶节点过多）而 “记住” 训练集噪声，泛化能力差。减少max_depth会限制树的复杂度，防止过度分裂，属于有效的正则化手段，能显著降低过拟合风险。\n",
    "# 4：决策树的分裂基于特征的 “阈值判断”（如 “x>5”），与特征的尺度无关（例如 “x=5 米” 和 “x=500 厘米” 对分裂无影响）。缩放特征（如标准化、归一化）对决策树的结构和性能无任何影响\n",
    "# 5：若特征数量从 m 变为 2m，其他条件不变，训练时间与 m 成正比，因此时间变为原来的 2 倍\n",
    "# 6：CART 算法复杂度为 O (n・m・log (n))，假设特征数 m 不变，时间与 n・log (n) 成正比。\n",
    "# 100 万实例（n₁=10⁶）：log₂(10⁶)≈20，因此 n₁・log (n₁)≈10⁶×20\n",
    "# 1000 万实例（n₂=10⁷）：log₂(10⁷)≈23.25，因此 n₂・log (n₂)≈10⁷×23.25\n",
    "# 时间比例为（10⁷×23.25）/（10⁶×20）≈10×1.16≈11.6，因此训练时间约为11.6 小时（大致 10-12 小时）。"
   ],
   "id": "e94fc641b843834e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 编程题",
   "id": "d57bc0d1695fa8ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. 为 新月形 数据集训练并微调一棵决策树。\n",
    "\n",
    "a. 使用make_moons(n_samples=10000，noise=0.4)生成一个 新月形 数据集。\n",
    "\n",
    "b. 使用train_test_split()拆分训练集和测试集。\n",
    "\n",
    "c. 使用交叉验证的网格搜索（在GridSearchCV类的帮助下）为Decision-TreeClassifier找到适合的超参数值。提示：尝试max_leaf_nodes的多种值。\n",
    "\n",
    "d. 使用超参数对整个训练集进行训练，并测量模型在测试集上的性能。你应该得到约85%～87%的准确率。"
   ],
   "id": "df859f669d173482"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. 按照以下步骤种植森林。\n",
    "\n",
    "a.继续之前的练习，生产1000个训练集子集，每个子集包含随机挑选的100个实例。提示：使用Scikit-Learn的ShuffleSplit类来实现。\n",
    "\n",
    "b.使用前面得到的最佳超参数值，在每个子集上训练一棵决策树。在测试集上评估这1000棵决策树。因为训练集更小，所以这些决策树的表现可能比第一棵决策树要差一些，只能达到约80%的精度。\n",
    "\n",
    "c.见证奇迹的时刻到了。用每个测试集实例，生成1000棵决策树的预测，然后仅保留次数最频繁的预测［可以使用SciPy的mode()函数］。这样你在测试集上可获得大多数投票的预测结果。\n",
    "\n",
    "d.评估测试集上的这些预测，你得到的准确率应该比第一个模型更高（高出0.5%～1.5%）。恭喜，你已经训练出了一个随机森林分类器！"
   ],
   "id": "ed6c04b5518d9498"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T02:33:55.754297Z",
     "start_time": "2025-08-18T02:33:55.742532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "# a. 生成新月形数据集（10000个样本，噪声0.4）\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "\n",
    "# b. 拆分训练集（80%）和测试集（20%）\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ],
   "id": "c73f731a299a3a99",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T02:33:28.002621Z",
     "start_time": "2025-08-18T02:33:22.070608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 定义决策树模型\n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# 定义超参数候选值（重点测试max_leaf_nodes）\n",
    "param_grid = {\n",
    "    'max_leaf_nodes': [None, 2, 5, 10, 20, 30, 50, 100, 200],  # 叶节点数量\n",
    "    'min_samples_split': [2, 5, 10],  # 拆分节点所需最小样本数（辅助调优）\n",
    "    'max_depth': [None, 5, 10, 20]  # 树的最大深度（辅助调优）\n",
    "}\n",
    "\n",
    "# 网格搜索（5折交叉验证）\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dtc,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1  # 利用所有CPU核心加速\n",
    ")\n",
    "\n",
    "# 在训练集上拟合\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 输出最优超参数\n",
    "print( grid_search.best_params_)\n",
    "print( grid_search.best_score_)"
   ],
   "id": "817b7b2c51eba38b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'max_leaf_nodes': 20, 'min_samples_split': 2}\n",
      "0.858625\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T02:33:31.917702Z",
     "start_time": "2025-08-18T02:33:31.903160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# c. 用最优超参数训练模型\n",
    "best_dtc = grid_search.best_estimator_\n",
    "\n",
    "# d. 在测试集上评估\n",
    "test_accuracy = best_dtc.score(X_test, y_test)\n",
    "test_accuracy"
   ],
   "id": "8b3a9ab926f445cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T02:34:40.195063Z",
     "start_time": "2025-08-18T02:34:36.983909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# a. 生成1000个训练集子集，每个包含100个随机挑选的实例\n",
    "from scipy.stats import mode\n",
    "n_trees = 1000\n",
    "n_instances = 100\n",
    "\n",
    "# 使用ShuffleSplit生成1000个随机子集，每个子集大小为100\n",
    "rs = ShuffleSplit(n_splits=n_trees, train_size=n_instances, random_state=42)\n",
    "\n",
    "# b. 在每个子集上训练决策树，并评估性能\n",
    "trees = []\n",
    "scores = []\n",
    "\n",
    "# 使用之前找到的最佳超参数\n",
    "best_params = {'max_leaf_nodes': 20, 'random_state': 42}\n",
    "\n",
    "for train_index, _ in rs.split(X_train):\n",
    "    X_subset = X_train[train_index]\n",
    "    y_subset = y_train[train_index]\n",
    "    tree = DecisionTreeClassifier(**best_params)\n",
    "    tree.fit(X_subset, y_subset)\n",
    "    trees.append(tree)\n",
    "\n",
    "    # 在测试集上评估\n",
    "    y_pred = tree.predict(X_test)\n",
    "    scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# 打印单个决策树的平均准确率\n",
    "print(f\"单个决策树的平均准确率: {np.mean(scores):.4f}\")\n",
    "print(f\"单个决策树的准确率标准差: {np.std(scores):.4f}\")\n",
    "\n",
    "# c. 生成多数投票预测结果\n",
    "# 初始化预测结果数组，形状为(测试集大小, 决策树数量)\n",
    "all_predictions = np.empty((len(X_test), n_trees))\n",
    "\n",
    "# 收集所有决策树的预测结果\n",
    "for i, tree in enumerate(trees):\n",
    "    all_predictions[:, i] = tree.predict(X_test)\n",
    "\n",
    "# 对每个测试实例，取多数投票结果\n",
    "# 使用mode函数获取最频繁的预测值\n",
    "y_pred_majority_votes, _ = mode(all_predictions, axis=1)\n",
    "\n",
    "# d. 评估多数投票的准确率\n",
    "# 将结果转换为一维数组\n",
    "y_pred_majority_votes = y_pred_majority_votes.reshape(-1)\n",
    "\n",
    "# 计算准确率\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred_majority_votes)\n",
    "print(f\"随机森林（多数投票）的准确率: {ensemble_accuracy:.4f}\")\n",
    "print(f\"提升幅度: {(ensemble_accuracy - np.mean(scores)):.4f}\")"
   ],
   "id": "708edc46da773fee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单个决策树的平均准确率: 0.8012\n",
      "单个决策树的准确率标准差: 0.0250\n",
      "随机森林（多数投票）的准确率: 0.8720\n",
      "提升幅度: 0.0708\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "14e546ef5c3fcabf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
