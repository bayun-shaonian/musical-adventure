{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 第一部分",
   "id": "c216d8cee0365df1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 简答题",
   "id": "c63b52b914e7bb74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. 为什么激活函数是训练一个多层感知机（MLP）的关键要素\n",
    "2. 列举三种常用的激活函数，说明一下它们的大概形状\n",
    "3. 反向传播的算法解决什么问题，如何工作的\n",
    "4. 列出可以在基本MLP（不考虑其他神经网络架构）中进行调整的所有超参数？如果MLP过拟合训练数据，如何调整这些超参数来解决该问题？\n",
    "5. 假设有一个MLP，该MLP由一个输入层，10个特征，随后是一个包含50个神经元的隐藏层，最后是3个神经元组成的输出层。所有人工神经元都使用ReLU激活函数。\n",
    "\n",
    "   a. 输入矩阵X的形状是什么\n",
    "\n",
    "   b. 隐藏层的权重W_hidden及其偏置b_hidden的形状分别是什么\n",
    "\n",
    "   c. 输出层的权重W_output及其偏置b_output的形状是什么\n",
    "\n",
    "   d. 网络输出矩阵Y的形状是什么\n",
    "\n",
    "   e. 写出输出矩阵Y的计算公式，满足Y是W_hidden, b_hidden, W_output, b_output的函数\n",
    "\n",
    "6. 如果要将电子邮件分类为垃圾邮件或正常邮件，需要在输出层中有多少个神经元？应该在输出层中使用什么激活函数？相反，如果想解决MNIST图片分类问题，则在输出层中需要有多少个神经元，应该使用哪种激活函数？如何使神经网络预测 回归话题里提到的房价？"
   ],
   "id": "efbfab39dff1fdac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1:激活函数的核心作用是为神经网络引入非线性变换。\n",
    "# 若没有激活函数，多层感知机本质上是多个线性层的叠加，而线性函数的组合仍然是线性函数，无法拟合复杂的非线性关系（如图像、文本中的特征关联）。\n",
    "# 激活函数通过非线性变换，使神经网络能够学习输入与输出之间的非线性映射，从而处理现实世界中具有复杂模式的任务（如分类、回归等）\n",
    "# 2:ReLU（Rectified Linear Unit）形状：分段线性函数，当输入 x>0时，输出 y=x；当 x≤0时，输出 y=0。图像呈折线状，在正区间保持线性，负区间被 \"截断\" 为 0。\n",
    "# Sigmoid形状：S 形曲线，输出范围为 (0,1)。当 x→+∞时，输出趋近于 1；当 x→−∞时，输出趋近于 0；在 x=0处输出为 0.5\n",
    "# Tanh（双曲正切）形状：类似 Sigmoid 的 S 形曲线，但输出范围为 (−1,1)，且关于原点对称。当 x→+∞时趋近于 1，x→−∞时趋近于 - 1；在 x=0处输出为 0。\n",
    "# 3:解决的问题：反向传播算法用于求解多层神经网络中参数（权重和偏置）的梯度，从而通过梯度下降法更新参数，最小化损失函数（即优化模型预测与真实标签的差距）。\n",
    "# 工作原理：前向传播：输入数据从输入层经隐藏层传递到输出层，计算模型的预测值，并通过损失函数（如交叉熵、MSE）计算预测值与真实标签的损失。反向传播：从输出层开始，利用链式法则反向计算损失函数对各层参数（权重和偏置）的梯度。具体而言，先计算输出层参数的梯度，再逐层传递到隐藏层和输入层，最终得到所有参数的梯度。参数更新：根据计算出的梯度，使用梯度下降法（或其变种）调整参数，降低损失（如w=w−η⋅∇w​L，其中η为学习率，∇w​L为损失对权重的梯度）。\n",
    "# 4:基本 MLP 的超参数结构相关：隐藏层数量、每个隐藏层的神经元数量；训练相关：学习率、批量大小（batch size）、训练轮数（epochs）、权重初始化方法（如随机初始化、Xavier 初始化）；\n",
    "# 正则化相关：L1/L2 正则化系数、Dropout 率（若使用 Dropout）；激活函数类型（如 ReLU、sigmoid 等）。\n",
    "# 过拟合时的调整策略过拟合指模型在训练数据上表现好，但泛化能力差（测试数据表现差），需通过降低模型复杂度或增强正则化解决：减少隐藏层数量或每个隐藏层的神经元数量（降低模型容量）；增大 L1/L2 正则化系数（限制权重大小，减少过拟合）；增大 Dropout 率（随机丢弃部分神经元，防止过度依赖特定特征）；减少训练轮数（提前停止训练，避免模型 \"记住\" 训练数据噪声）；适当减小批量大小（增加参数更新频率，可能增强泛化能力）。\n",
    "# 5:a. 输入矩阵 X 的形状\n",
    "# 设样本数量为m，输入特征数为 10，则X的形状为（m,10)（m行表示样本，10 列表示特征）。\n",
    "# b. 隐藏层的权重 W_hidden 及偏置 b_hidden 的形状Whidden：连接输入层（10 个特征）与隐藏层（50 个神经元），形状为(10,50)（输入特征数 × 隐藏层神经元数）。bhidden\n",
    "# ：每个隐藏层神经元对应一个偏置，形状为(1,50)（可通过广播适配样本维度）。\n",
    "# c. 输出层的权重 W_output 及偏置 b_output 的形状Woutput\n",
    "# ：连接隐藏层（50 个神经元）与输出层（3 个神经元），形状为(50,3)（隐藏层神经元数 × 输出层神经元数）。boutput\n",
    "# ：每个输出层神经元对应一个偏置，形状为(1,3)。\n",
    "# d. 网络输出矩阵 Y 的形状输出层有 3 个神经元，样本数为m，故 的形状为(m,3)。\n",
    "# e. 输出矩阵 Y 的计算公式隐藏层输入：zhidden=X⋅Whidden+bhidden（矩阵乘法 + 偏置广播）；隐藏层输出（ReLU 激活）：ahidden=ReLU(zhidden)；输出层输入：zoutput=ahidden⋅Woutput+boutput；网络输出（ReLU 激活）：Y=ReLU(zoutput)。整合公式：Y=ReLU(ReLU(X⋅Whidden+bhidden)⋅Woutput+boutput)\n",
    "# 6:电子邮件分类（垃圾邮件 / 正常邮件）\n",
    "# 输出层神经元数量：1 个（二分类问题，输出表示属于 \"垃圾邮件\" 的概率）。\n",
    "# 激活函数：sigmoid（输出范围 (0,1)，可直接解释为概率，如 > 0.5 预测为垃圾邮件）。\n",
    "# MNIST 图片分类（10 类数字）\n",
    "# 输出层神经元数量：10 个（对应 10 个数字类别）。\n",
    "# 激活函数：softmax（将输出转换为概率分布，每个神经元输出表示属于对应类别的概率，且所有概率和为 1）。\n",
    "# 房价预测（回归任务）\n",
    "# 输出层神经元数量：1 个（预测单一连续值 —— 房价）。\n",
    "# 激活函数：无需激活函数（或线性激活），因为房价是连续值，无需限制输出范围，直接输出线性组合结果即可。"
   ],
   "id": "1c42147e5f123676"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 编程题",
   "id": "3a345ebdc81ebc04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在MNIST数据集上训练深度MLP（可以使用tf.keras.datasets.mnist.load_data()加载它）。看看是否可以通过手动调整超参数获得98%以上的精度。\n",
    "\n",
    "首先尝试使用课堂上介绍的方法搜索最佳学习率（即通过以指数方式增加学习率，根据学习率变化绘制训练损失，并找到损失激增的点）。\n",
    "\n",
    "接下来，尝试使用Keras Tuner调整超参数——保存检查点，使用早停，并使用TensorBoard绘制学习曲线。"
   ],
   "id": "8ae50ffff27ba189"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import kerastuner as kt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 加载MNIST数据集\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 数据预处理\n",
    "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# 创建保存日志和模型的目录\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# 学习率搜索：指数增加学习率并绘制损失曲线\n",
    "def find_optimal_learning_rate():\n",
    "    # 构建基础模型\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(256, activation='relu', input_shape=(784,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # 编译模型\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # 学习率调度器：指数增加学习率\n",
    "    lr_scheduler = callbacks.LearningRateScheduler(\n",
    "        lambda epoch: 1e-6 * 10**(epoch / 20)\n",
    "    )\n",
    "\n",
    "    # 训练模型以寻找最佳学习率\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[lr_scheduler],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 绘制学习率与损失的关系\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogx(history.history['lr'], history.history['loss'], color='blue', linewidth=2)\n",
    "    plt.xlabel('学习率', fontsize=12)\n",
    "    plt.ylabel('损失', fontsize=12)\n",
    "    plt.title('学习率与损失关系图', fontsize=14)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig('learning_rate_vs_loss.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # 找到损失开始激增的点作为最佳学习率\n",
    "    loss_values = history.history['loss']\n",
    "    lr_values = history.history['lr']\n",
    "    min_loss_idx = np.argmin(loss_values)\n",
    "    optimal_lr = lr_values[min_loss_idx]\n",
    "\n",
    "    print(f\"找到的最佳学习率: {optimal_lr:.6f}\")\n",
    "    return optimal_lr\n",
    "\n",
    "# 搜索最佳学习率\n",
    "optimal_lr = find_optimal_learning_rate()\n",
    "\n",
    "# 定义用于Keras Tuner的模型构建函数\n",
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # 输入层\n",
    "    model.add(layers.Dense(784, input_shape=(784,)))\n",
    "\n",
    "    # 隐藏层 - 可调节层数和每层神经元数\n",
    "    for i in range(hp.Int('num_layers', 1, 4)):\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32),\n",
    "            activation='relu'\n",
    "        ))\n",
    "\n",
    "        # 可选的Dropout层\n",
    "        if hp.Boolean(f'dropout_{i}'):\n",
    "            model.add(layers.Dropout(hp.Float(f'dropout_rate_{i}', 0.1, 0.5, step=0.1)))\n",
    "\n",
    "    # 输出层\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # 编译模型\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice('learning_rate',\n",
    "                                  values=[optimal_lr * 0.1, optimal_lr, optimal_lr * 10])\n",
    "        ),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# 初始化Keras Tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='mnist_tuner',\n",
    "    project_name='mnist_mlp'\n",
    ")\n",
    "\n",
    "# 搜索最佳超参数组合\n",
    "tuner.search(\n",
    "    x_train, y_train,\n",
    "    epochs=20,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(patience=3, monitor='val_loss')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 获取最佳模型\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# 定义回调函数\n",
    "callbacks_list = [\n",
    "    # 早停策略\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    # 模型检查点\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, 'best_model.h5'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    # TensorBoard日志\n",
    "    callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_images=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# 使用最佳模型进行最终训练\n",
    "history = best_model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks_list\n",
    ")\n",
    "\n",
    "# 在测试集上评估模型\n",
    "test_loss, test_acc = best_model.evaluate(x_test, y_test)\n",
    "print(f\"测试集准确率: {test_acc:.4f}\")\n",
    "\n",
    "# 绘制训练过程中的准确率曲线\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# 准确率曲线\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='训练准确率')\n",
    "plt.plot(history.history['val_accuracy'], label='验证准确率')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('准确率')\n",
    "plt.title(f'最终测试准确率: {test_acc:.4f}')\n",
    "plt.legend()\n",
    "\n",
    "# 损失曲线\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='训练损失')\n",
    "plt.plot(history.history['val_loss'], label='验证损失')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('损失')\n",
    "plt.title('训练与验证损失')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png')\n",
    "plt.show()\n",
    "\n",
    "# 打印最佳超参数\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"最佳超参数:\")\n",
    "for hp in best_hps.values:\n",
    "    print(f\"{hp}: {best_hps.get(hp)}\")\n"
   ],
   "id": "42021e54f2fa543",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 第二部分",
   "id": "aa17e7e1e0f2258c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "第二部分要求构建一个基本的softmax回归算法，以及一个简单的两层神经网络。将使用原生Python（使用numpy库），不借助keras实现这些算法\n",
    "\n",
    "在此过程中，将提供一些关于如何实现这些不同函数的指导，但总体而言，细节需要自己实现。 应该尽量使用 numpy 中的线性代数调用：for/while循环通常会使代码运行速度比预期慢得多。\n",
    "\n",
    "**请仔细阅读作业说明!!!**"
   ],
   "id": "6c3f1fe62132569f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "去命令行（cmd/Anaconda Powershell Prompt /其他终端）运行如下指令（激活开发环境一定要最先执行），安装这部分作业依赖的python库：\n",
    "- 激活开发环境：conda activate homl3\n",
    "- 安装numdifftools：conda install numdifftools\n",
    "- 安装pytest：conda install pytest\n"
   ],
   "id": "bbec9387c82d28a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 第一题：简单的加法函数，以及使用pytest测试代码",
   "id": "4c898f53cabb194f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "为了说明这部分作业的代码+数据，以及pytest使用，将使用一个实现 add函数 的简单示例。\n",
    "\n",
    "```\n",
    "data/\n",
    "    train-images-idx3-ubyte.gz\n",
    "    train-labels-idx1-ubyte.gz\n",
    "    t10k-images-idx3-ubyte.gz\n",
    "    t10k-labels-idx1-ubyte.gz\n",
    "src/\n",
    "    simple_nn.py\n",
    "tests/\n",
    "    test_simple_nn.py\n",
    "```\n",
    "\n",
    "data/ 目录包含这部分作业所需的数据（MNIST 数据集的副本）；src/ 目录包含实现功能所需的源代码；tests/ 目录包含用于测试实现代码是否正确的代码\n",
    "\n",
    "第一题要求实现 src/目录里 simple_nn.py内的 add函数（这个简单的函数实际上并没有用到，它只是一个帮助熟悉作业结构的示例）。查看 src/simple_nn.py 文件，将找到 add() 函数的定义"
   ],
   "id": "e9c91dfa3c0912a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "def add(x, y):\n",
    "    \"\"\"一个简单的add函数，以便熟悉自动测试（pytest）\n",
    "\n",
    "    Args:\n",
    "        x (Python数字 或者 numpy array)\n",
    "        y (Python数字 或者 numpy array)\n",
    "\n",
    "    Return:\n",
    "        x+y的和\n",
    "    \"\"\"\n",
    "    ### 你的代码开始\n",
    "    return x+y\n",
    "    ### 你的代码结束\n",
    "```\n",
    "\n",
    "函数内的文档字符串（docstring）定义了函数应该产生的预期输入和输出（需要养成仔细阅读文档的习惯，很多错误来源就是没有阅读规范）。实现这个函数。你只需将 pass 语句替换为正确的代码即可，即：\n",
    "\n",
    "```python\n",
    "def add(x, y):\n",
    "    \"\"\"一个简单的add函数，以便熟悉自动测试（pytest）\n",
    "\n",
    "    Args:\n",
    "        x (Python数字 或者 numpy array)\n",
    "        y (Python数字 或者 numpy array)\n",
    "\n",
    "    Return:\n",
    "        x+y的和\n",
    "    \"\"\"\n",
    "    ### 你的代码开始\n",
    "    return x + y\n",
    "    ### 你的代码结束\n",
    "```\n",
    "\n",
    "现在可以去src/simple_nn.py里，把add函数里的pass 换成 return x + y"
   ],
   "id": "b968917c2ee80ca7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 测试代码",
   "id": "919bb56f32b06378"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "现在需要测试一下你的代码是否能正确运行，正确运行才说明实现没问题。\n",
    "\n",
    "在这部分作业中，将使用pytest对代码进行单元测试。在 src/simple_nn.py 文件中 写完 add函数的实现后，去命令行里确保已经激活了homl3环境（conda activate homl3）， 确保homl3环境里安装过了numdifftools和pytest，确定命令行里显示的文件路径在 作业8的目录（这个目录同时有data/, src/和tests/文件夹），然后执行以下命令：\n",
    "\n",
    "python -m pytest -k \"add\"\n",
    "\n",
    "如果一切正常，你会看到类似这样的图片：\n",
    "![测试add通过](../../images/homework/neural_network/p1.png)\n",
    "\n",
    "想看测试如何进行的，可以去查看tests/test_simple_nn.py文件，python -m pytest -k \"add\"指令刚刚运行的是 文件里的test_add() 函数\n",
    "\n",
    "如果错误地实现了某些内容（例如，将上面的 x + y 更改为 x - y），那么测试将会失败，并且 pytest 将会指示相应的测试失败。\n",
    "\n",
    "比如把x+y，换成x-y后，执行python -m pytest -k \"add\"：\n",
    "![测试add不通过](../../images/homework/neural_network/p2.png)"
   ],
   "id": "8cd29340f4d80aac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "如图所见，将收到一个错误，指示断言失败，然后就可以使用它来返回并调整实现代码。应该能够熟练地阅读和跟踪测试文件，以便更好地理解正确的实现应该如何工作\n",
    "\n",
    "学习正确开发和使用单元测试对于现代软件开发至关重要，希望这次作业帮助了解单元测试在软件开发中的典型用法。\n",
    "\n",
    "当然，这次作业不一定需要编写自己的测试去确保自己实现正确，但应该熟悉如何阅读提供的测试文件，以便了解要实现的函数应该如何运行。但是，也绝对鼓励为自己的实现编写额外的测试。\n",
    "\n",
    "如果习惯通过打印语句调试代码，请注意，pytest 默认会捕获任何输出（隐藏掉测试代码执行的print）。可以通过将 -s 传递给 pytest 来禁用此行为并让测试在所有情况下显示所有输出。"
   ],
   "id": "33304ba8ce802fd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 第二题：用gzip和struct处理压缩文件和二进制数据，加载MNIST数据",
   "id": "ebec5034752b27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "现在已经熟悉了测试工具pytest，接下来在 src/simple_nn.py 中需要实现的函数上尝试一下：parse_mnist_data() 函数。\n",
    "\n",
    "这个函数也有文档字符串（docstring），请仔细阅读它们。\n",
    "\n",
    "然后，请访问 https://web.archive.org/web/20220509025752/http://yann.lecun.com/exdb/mnist/ 了解 MNIST 数据的二进制格式。然后编写函数读取此类文件，并根据文档字符串中的规范返回 numpy 数组）。建议使用 Python 中的 struct 模块（以及 gzip 模块，当然还有 numpy 本身）来实现此函数。\n",
    "\n",
    "当然可以利用AI搜索这个部分的代码实现，但了解了MNIST数据的二进制格式和gzip，struct的简单使用后，能理解AI产出的代码为什么正确\n",
    "\n",
    "实现函数后，去命令行运行本地单元测试， 同样确保命令行激活了homl3环境，确保路径在作业8目录下（有data/,src/和tests/ 文件夹）， 后面的题不再强调\n",
    "\n",
    "python -m pytest -k \"parse_mnist\""
   ],
   "id": "f22b95e3905b669b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 第三题：Softmax损失",
   "id": "1323562d0cba4cfa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在 `src/simple_nn.py` 文件的 `softmax_loss()` 函数中实现 softmax（也称为交叉熵）损失。对于一个可以取值 $ y \\in \\{1, \\ldots, k\\} $ 的多类输出，softmax 损失接收一个对数几率向量 $ z \\in \\mathbb{R}^k $ 和真实类别 $ y \\in \\{1, \\ldots, k\\} $ 作为输入，并返回由以下公式定义的损失：\n",
    "\n",
    "$\\ell_{\\text{softmax}}(z, y) = \\log (\\sum_{i=1}^{k} \\exp z_i) - z_y$\n",
    "\n",
    "对数几率向量z，可以看成被softmax激活之前的值，对公式有疑惑，或者对z的意义有疑惑的，可以参考softmax回归的笔记，并自己推导一下损失公式是否正确\n",
    "\n",
    "请注意，如其文档字符串（docstring）所述，`softmax_loss()` 函数接收一个二维的对数几率数组（即，一批不同样本的 $ k $ 维对数几率）加上一个对应的一维真实标签数组，并应返回整批样本的平均 softmax 损失。请注意，为了正确实现此功能，你不应使用任何循环，而是完全使用 numpy 的向量化操作进行计算（为此设定预期，实现代码可以少到一行代码）。"
   ],
   "id": "3cdb52032ac61c62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "实现完成后，可以去命令行进行单元测试：python -m pytest -k \"softmax_loss\"",
   "id": "413eec903944c28e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 第四题：softmax回归小批量梯度下降",
   "id": "a3f4e9cefaaa8dc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在这个问题中，你将实现（线性）softmax 回归的小批量梯度下降）。考虑一个假设函数，该函数通过以下公式将 $ n $ 维输入转换为 $ k $ 维对数几率：\n",
    "\n",
    "$h(x) = \\Theta^T x$\n",
    "\n",
    "其中 $ x \\in \\mathbb{R}^n $ 是输入，$\\Theta \\in \\mathbb{R}^{n \\times k}$ 是模型参数。给定数据集 $\\{(x^{(i)} \\in \\mathbb{R}^n, y^{(i)} \\in \\{1, \\ldots, k\\})\\}$，其中 $ i = 1, \\ldots, m $，softmax 回归相关的优化问题因此由下式给出：\n",
    "\n",
    "$\\text{minimize} \\frac{1}{m} \\sum_{i=1}^{m} \\ell_{\\text{softmax}} (\\Theta^T x^{(i)}, y^{(i)})$\n",
    "\n",
    "线性 softmax 目标的梯度由下式给出，有疑惑的可以结合softmax回归的笔记验证\n",
    "\n",
    "$\\nabla_\\Theta \\ell_{\\text{softmax}} (\\Theta^T x, y) = x(z - e_y)^T$\n",
    "\n",
    "其中\n",
    "\n",
    "$z = \\frac{\\exp(\\Theta^T x)}{1^T \\exp(\\Theta^T x)} = \\text{normalize}(\\exp(\\Theta^T x))$\n",
    "\n",
    "（即 $ z $ 只是归一化的 softmax 概率），并且 $ e_y $ 表示 y 分类的独热编码，即一个所有元素为零，只有第 $ y $ 个位置为 1 的向量。\n",
    "\n",
    "也可以用更紧凑的符号来表示，方便代码实现，即，如果让 $ X \\in \\mathbb{R}^{m \\times n} $ 表示某个 $ m $ 个输入的特征矩阵（整个数据集或一个小批量），$ y \\in \\{1, \\ldots, k\\}^m $ 是对应的标签向量，并且 $ \\ell_{\\text{softmax}} $ 表示平均 softmax 损失，那么\n",
    "\n",
    "$\\nabla_\\Theta \\ell_{\\text{softmax}}(X \\Theta, y) = \\frac{1}{m} X^T (Z - I_y)$\n",
    "\n",
    "其中\n",
    "\n",
    "$Z = \\text{normalize}(\\exp(X \\Theta)) \\quad (\\text{归一化按行应用})$\n",
    "\n",
    "表示对数几率矩阵，而 $ I_y \\in \\mathbb{R}^{m \\times k} $ 表示 $ y $ 中标签的 逐个转成 独热编码，按行连接\n",
    "\n",
    "使用这些梯度，实现 `softmax_regression_epoch()` 函数，该函数使用指定的学习率/步长 $ \\eta $ 和小批量大小 `batch_size` 运行单个轮次（对数据集的一次遍历）。如其文档字符串所述，你的函数应该就地修改 Theta 数组。实现后，请去命令行运行测试。\n",
    "\n",
    "python -m pytest -k \"softmax_regression_epoch\""
   ],
   "id": "67f005120ef5a202"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 用softmax回归训练MNIST",
   "id": "f0e6a75ce062199d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "虽然这不包含在测试中，但既然你已经编写了这段代码，你也可以尝试使用 SGD 训练一个完整的 MNIST 线性分类器。为此，你可以使用 src/simple_nn.py 文件中的 train_softmax() 函数（已经编写好了这个函数，所以无需自行编写，但可以查看一下它的功能）。\n",
    "\n",
    "可以使用以下代码了解它的工作原理。作为参考，如下所示，我的实现在 notebook 上运行时间约为 2 秒，测试集错误率为 7.97%。"
   ],
   "id": "fc5478e35ebb74bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(\"src/\")\n",
    "from simple_nn import train_softmax, parse_mnist\n",
    "\n",
    "X_tr, y_tr = parse_mnist(\"data/train-images-idx3-ubyte.gz\",\n",
    "                         \"data/train-labels-idx1-ubyte.gz\")\n",
    "X_te, y_te = parse_mnist(\"data/t10k-images-idx3-ubyte.gz\",\n",
    "                         \"data/t10k-labels-idx1-ubyte.gz\")\n",
    "\n",
    "train_softmax(X_tr, y_tr, X_te, y_te, epochs=10, lr=0.2, batch=100)"
   ],
   "id": "2a7bf41f4f1f5211",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 在1个隐藏层的神经网络上进行小批量梯度下降",
   "id": "6e72acd86987b24e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "现在已经为线性分类器编写了SGD，现在考虑一个简单的两层神经网络的情况。具体来说，对于输入 $ x \\in \\mathbb{R}^n $，考虑一个形式如下的两层神经网络（无偏置项）：\n",
    "\n",
    "$ z = W_2^T ReLU(W_1^T x) $\n",
    "\n",
    "其中 $ W_1 \\in \\mathbb{R}^{n \\times d} $ 和 $ W_2 \\in \\mathbb{R}^{d \\times k} $ 表示网络的权重（具有 $ d $ 维隐藏单元），而 $ z \\in \\mathbb{R}^k $ 表示网络输出的对数几率。我们再次使用 softmax/交叉熵损失，这意味着我们要解决以下优化问题：\n",
    "\n",
    "$\\text{minimize } \\frac{1}{W_1, W_2} \\sum_{i=1}^m \\ell_{\\text{softmax}}(W_2^T ReLU(W_1^T x^{(i)}), y^{(i)})$\n",
    "\n",
    "或者，使用矩阵 $ X \\in \\mathbb{R}^{m \\times n} $ 来描述批量形式，这也可以写成：\n",
    "\n",
    "$\\text{minimize } \\ell_{\\text{softmax}}(ReLU(XW_1)W_2, y)$\n",
    "\n",
    "使用链式法则，可以推导出该网络的反向传播更新（为了便于实现，这里提供最终形式）。具体来说，令：\n",
    "\n",
    "$Z_1 \\in \\mathbb{R}^{m \\times d} = ReLU(XW_1)$\n",
    "\n",
    "$G_2 \\in \\mathbb{R}^{m \\times k} = \\text{normalize}(\\exp(Z_1 W_2)) - I_y$\n",
    "\n",
    "$G_1 \\in \\mathbb{R}^{m \\times d} = 1\\{Z_1 > 0\\} \\circ (G_2 W_2^T)$\n",
    "\n",
    "其中 $ 1\\{Z_1 > 0\\} $ 是一个二进制矩阵，其条目根据 $ Z_1 $ 中的每个项是否严格为正而等于零或一，而 $\\circ$ 表示逐元素乘法。那么目标的梯度由下式给出：\n",
    "\n",
    "$\\nabla_{W_1} \\ell_{\\text{softmax}}(ReLU(XW_1)W_2, y) = \\frac{1}{m} X^T G_1$\n",
    "\n",
    "$\\nabla_{W_2} \\ell_{\\text{softmax}}(ReLU(XW_1)W_2, y) = \\frac{1}{m} Z_1^T G_2$\n",
    "\n",
    "**注意：** 如果这些精确方程的细节对你来说有点神秘，不必太担心。这些只是两层ReLU网络的标准反向传播方程：$ Z_1 $ 项只是计算\"前向\"传播，而 $ G_2 $ 和 $ G_1 $ 项表示反向传播。但是更新的精确形式可能会因你使用的神经网络符号、制定损失函数的具体方式、是否之前以矩阵形式推导过这些等因素而有所不同。（毕竟，在某种程度上，深度学习系统（比如tensorflow）的整个重点是我们不需要费心进行这些手动计算）。\n"
   ],
   "id": "f6a35e81834a15a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "使用这些梯度，现在在 src/simple_nn.py 文件中编写 nn_epoch() 函数。与上一个问题一样，你的解决方案应该修改 W1 和 W2 数组。实现该函数后，运行以下测试。请务必使用上述表达式所示的矩阵运算来实现该函数：这比尝试使用循环更快、更高效（并且所需的代码也更少）。\n",
    "\n",
    "实现完成后，去命令运行单元测试：python -m pytest -k \"nn_epoch\""
   ],
   "id": "53c19e9d5f2d9c32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 训练神经网络",
   "id": "ff593cbba822cafb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "import importlib\n",
    "import simple_nn\n",
    "importlib.reload(simple_nn) # 重新载入simple_nn， 防止刚才的训练代码产生了缓存，影响了simple_nn\n",
    "\n",
    "sys.path.append(\"src/\")\n",
    "from simple_nn import train_nn, parse_mnist\n",
    "\n",
    "X_tr, y_tr = parse_mnist(\"data/train-images-idx3-ubyte.gz\",\n",
    "                         \"data/train-labels-idx1-ubyte.gz\")\n",
    "X_te, y_te = parse_mnist(\"data/t10k-images-idx3-ubyte.gz\",\n",
    "                         \"data/t10k-labels-idx1-ubyte.gz\")\n",
    "train_nn(X_tr, y_tr, X_te, y_te, hidden_dim=400, epochs=20, lr=0.2)"
   ],
   "id": "3f41ff4fe2309f37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "作为参考，我的实现花了30多秒训练，最终在mnist的测试集达到了1.93%的错误率，只用了大概20多行代码",
   "id": "24c53df020c227d7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
